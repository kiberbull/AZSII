# Практическое задание № 5: PGD-Атака на модель MNIST

## Цель
Демонстрация использования PGD-атаки (Projected Gradient Descent) для проверки устойчивости обученной модели MNIST к противоречивым примерам.

---

## Результаты

### 1. Исходные данные и модель
![Исходные данные и модель](https://github.com/user-attachments/assets/d1addcd9-4ac8-49dd-907d-60f88558113d)

---

### 2. Преобразование данных
![Преобразование данных](https://github.com/user-attachments/assets/47788439-902b-4db7-b5ab-8b08d6f7bf2a)

---

### 3. Реализация атаки PGD
![Реализация атаки PGD](https://github.com/user-attachments/assets/b1bcce68-2114-4675-95d3-70b88d5377c6)

---

### 4. Визуализация результатов
#### Оригинальное изображение и противоречивое изображение (PGD)
![Оригинальное и противоречивое изображение](https://github.com/user-attachments/assets/64226af2-4764-4b65-b278-3ceebbbb377a)

---

### 5. Итоговая оценка точности модели
![Итоговая оценка точности](https://github.com/user-attachments/assets/3748f678-2d10-422f-801d-ffa0dbbb2133)

---

## Вывод

PGD-атака успешно продемонстрировала, как модель, обученная на данных MNIST, теряет устойчивость к небольшим искажениям в изображениях. Это подчеркивает важность разработки методов повышения устойчивости моделей машинного обучения перед целенаправленными атаками.
