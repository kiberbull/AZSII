# Практическое задание №4: Атака DeepFool на модели ИИ

## Описание
В данном задании реализована атака DeepFool на нейронную сеть, обученную на датасете MNIST. Цель атаки — проверить устойчивость модели к искажениям в данных и оценить снижение точности классификации на атакованных примерах.

---

![image](https://github.com/user-attachments/assets/7703b3fe-e89b-4ab9-ac81-2447a66ab102)

## Шаг 1: Загрузка обученной модели и данных MNIST

![image](https://github.com/user-attachments/assets/00e2f6c0-7619-4558-9e2f-493864dd2818)

На этом этапе производится загрузка предварительно обученной модели (`mnist_model.h5`) и данных из датасета MNIST. Изображения нормализуются, а метки преобразуются в формат one-hot encoding для корректной работы с атакой.

---

## Шаг 2: Реализация атаки DeepFool с использованием Foolbox

![image](https://github.com/user-attachments/assets/83663fef-a9bb-4dfe-aaa4-8ec2397f5dc2)


Для проведения атаки используется библиотека `ART` с реализацией атаки DeepFool. Классификатор модели создается с помощью `TensorFlowV2Classifier`, а затем применяется атака на первые 100 изображений тестового набора.

---

## Шаг 3: Оценка модели на противоречивых примерах

![image](https://github.com/user-attachments/assets/c83a5655-85b8-4bfe-88c3-9396d385d642)

После проведения атаки оценивается точность модели на оригинальных и атакованных данных. Также рассчитываются потери для обоих наборов изображений.

---

## Шаг 4: Реализация атаки DeepFool на первых 100 изображениях

![image](https://github.com/user-attachments/assets/9a4aee87-5950-4149-8eaf-f9d31a9b9f56)

Атакованные изображения визуализируются для наглядного представления влияния атаки. Сравниваются оригинальные и атакованные изображения.

---

## Вывод

1. **Точность модели на оригинальных изображениях**: 98-100% (в зависимости от конфигурации модели и выборки данных).
2. **Точность на атакованных изображениях**: Резко падает до ~0-5%, что указывает на значительную уязвимость модели к атакам DeepFool.
3. **Визуальный эффект**: Искажения на атакованных изображениях минимальны и практически не заметны для человеческого глаза, но достаточны для изменения предсказаний модели.

---

## Итоговые результаты

### Оригинальное и Атакованное изображение

![image](https://github.com/user-attachments/assets/99884eb8-8a81-41c4-be80-48b5d0a7047b)

Данная работа демонстрирует, насколько важна разработка устойчивых к атакам моделей, способных обрабатывать данные с минимальными искажениями. Это может быть критически важно для систем безопасности и других приложений, где требуется высокая надежность классификации.
